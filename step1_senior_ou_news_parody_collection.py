import os
import feedparser
from datetime import datetime, timedelta
from anthropic import Anthropic
from anthropic._exceptions import OverloadedError, APIError
from dotenv import load_dotenv
from common_utils import get_gsheet, get_gspread_client, get_kst_now
from difflib import SequenceMatcher
import json
import re
from pathlib import Path
import time
import gspread
import pandas as pd
from newspaper import Article, Config  # newspaper3k ì„¤ì¹˜ í•„ìš”: pip install newspaper3k
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Any, Dict, Optional
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# .env íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ì§€ì •í•˜ì—¬ ë¡œë“œ
env_path = Path('.') / '.env'
load_dotenv(dotenv_path=env_path, verbose=True)

# Claude AI API í‚¤
CLAUDE_API_KEY = os.getenv('CLAUDE_API_KEY')
if not CLAUDE_API_KEY:
    raise ValueError("CLAUDE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì˜ í˜„ì¬ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
SCRIPT_DIR = Path(__file__).resolve().parent

def parse_rawdata(file_path='asset/rawdata.txt') -> Dict[str, Any]:
    """rawdata.txt íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ ì„¤ì •ê°’ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    config: Dict[str, Any] = {'rss_urls': []}
    current_section = None
    
    if not Path(file_path).exists():
        logger.error(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return config
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                if line.startswith('[') and line.endswith(']'):
                    current_section = line[1:-1]
                elif current_section == 'ì—°í•©ë‰´ìŠ¤RSS':
                    config['rss_urls'].append(line)
                elif ':' in line:
                    key, value = line.split(':', 1)
                    k = key.strip()
                    if k == 'rss_urls':
                        continue
                    config[k] = value.strip()
    except Exception as e:
        logger.error(f"ì„¤ì • íŒŒì¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {}
    return config

# êµ¬ê¸€ ì‹œíŠ¸ ì„¤ì •
WRITE_SHEET_NAME = 'senior_ou_news_parody_v3'
DISCLAIMER = "ë©´ì±…ì¡°í•­ : íŒ¨ëŸ¬ë””/íŠ¹ì •ê¸°ê´€,ê°œì¸ê³¼ ë¬´ê´€/íˆ¬ìì¡°ì–¸ì•„ë‹˜/ì¬ë¯¸ëª©ì "
# SOURCE_PREFIX = "https://gnews/" # ì¶œì²˜ URL ì ‘ë‘ì‚¬  # ì‚­ì œ

def get_article_content(url: str) -> Optional[Dict[str, Any]]:
    """ì£¼ì–´ì§„ URLì˜ ë‰´ìŠ¤ ë³¸ë¬¸ì„ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤."""
    try:
        user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'
        config = Config()
        config.browser_user_agent = user_agent
        config.request_timeout = 10
        
        article = Article(url, config=config)
        article.download()
        article.parse()
        
        if not article.text or len(article.text.strip()) < 100:
            logger.warning(f"ê¸°ì‚¬ ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŒ: {url}")
            return None
            
        return {
            'url': url, 
            'title': article.title, 
            'text': article.text, 
            'publish_date': article.publish_date
        }
    except Exception as e:
        logger.warning(f"ê¸°ì‚¬ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {e}")
        return None

def fetch_news_from_rss(rss_urls: List[str]) -> List[Dict[str, Any]]:
    """ì—¬ëŸ¬ RSS í”¼ë“œì—ì„œ ìµœì‹  ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    all_entries = []
    for url in rss_urls:
        logger.info(f"RSS í”¼ë“œ í™•ì¸ ì¤‘: {url}")
        try:
            feed = feedparser.parse(url)
            if hasattr(feed, 'status') and isinstance(feed.status, int) and feed.status >= 400:
                logger.warning(f"RSS í”¼ë“œ ì˜¤ë¥˜ (HTTP {feed.status}): {url}")
                continue
                
            for entry in feed.entries:
                entry['source_rss'] = url
                all_entries.append(entry)
        except Exception as e:
            logger.error(f"RSS í”¼ë“œ íŒŒì‹± ì˜¤ë¥˜: {url}, {e}")
            continue
            
    # ì¤‘ë³µ ì œê±° (link ê¸°ì¤€)
    unique_entries = list({entry.link: entry for entry in all_entries}.values())
    logger.info(f"ì´ {len(unique_entries)}ê°œì˜ ê³ ìœ í•œ ë‰´ìŠ¤ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
    return unique_entries

def rank_and_select_news(news_list: List[Dict[str, Any]], num_to_select: int = 30) -> List[Dict[str, Any]]:
    """ì‹œë‹ˆì–´ì¸µ(50-70ëŒ€) ê´€ì‹¬ë„ ê¸°ë°˜ ë‰´ìŠ¤ ì„ ì •"""
    # 50/60/70ëŒ€ íƒ€ê²Ÿìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì¬ì¡°ì •
    SENIOR_CATEGORY_WEIGHTS = {
        'health': 2.5, 'welfare': 2.3, 'economy': 2.0, 'politics': 1.8,
        'opinion': 1.6, 'local': 1.5, 'market': 1.4, 'society': 1.3,
    }
    
    # 50/60/70ëŒ€ í•µì‹¬ ê´€ì‹¬ì‚¬ë¡œ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ê°•í™”
    SENIOR_KEYWORD_WEIGHTS = {
        # ì—°ê¸ˆ/ë³µì§€ ê´€ë ¨ (ìµœê³  ìš°ì„ ìˆœìœ„)
        'ì—°ê¸ˆ': 15, 'êµ­ë¯¼ì—°ê¸ˆ': 14, 'ê¸°ì´ˆì—°ê¸ˆ': 13, 'ë…¸ë ¹ì—°ê¸ˆ': 12,
        'ì˜ë£Œë¹„': 12, 'ê±´ê°•ë³´í—˜': 12, 'ìš”ì–‘ë³´í—˜': 11, 'ì¥ê¸°ìš”ì–‘': 10,
        
        # ê±´ê°• ê´€ë ¨ (50/60/70ëŒ€ í•µì‹¬ ê´€ì‹¬ì‚¬)
        'ì¹˜ë§¤': 12, 'ê±´ê°•ê²€ì§„': 10, 'ê³ í˜ˆì••': 9, 'ë‹¹ë‡¨': 9, 'ì•”': 9,
        'ê´€ì ˆ': 8, 'ë¬´ë¦': 8, 'í—ˆë¦¬': 8, 'ë°±ë‚´ì¥': 7, 'ê³¨ë‹¤ê³µì¦': 7,
        
        # ê²½ì œ/ìƒí™œ ê´€ë ¨
        'ë¬¼ê°€': 11, 'ì „ê¸°ë£Œ': 10, 'ê°€ìŠ¤ìš”ê¸ˆ': 10, 'ìˆ˜ë„ìš”ê¸ˆ': 9,
        'ë¶€ë™ì‚°': 8, 'ì§‘ê°’': 8, 'ì•„íŒŒíŠ¸': 7, 'ì „ì„¸': 7, 'ì„ëŒ€ë£Œ': 7,
        'ê¸ˆë¦¬': 8, 'ì˜ˆê¸ˆ': 7, 'ì ê¸ˆ': 6, 'í€ë“œ': 5, 'ì£¼ì‹': 6,
        
        # ì •ì¹˜/ì‚¬íšŒ ê´€ë ¨
        'ëŒ€í†µë ¹': 9, 'ì •ë¶€': 8, 'êµ­ì •ê°ì‚¬': 7, 'íŠ¹ê²€': 7, 'êµ­íšŒ': 6,
        'ì„¸ê¸ˆ': 9, 'ì†Œë“ì„¸': 8, 'ì¬ì‚°ì„¸': 8, 'ìƒì†ì„¸': 7,
        
        # ë…¸ì¸ë³µì§€ ê´€ë ¨
        'ë…¸ì¸ë³µì§€': 12, 'ë…ê±°ë…¸ì¸': 10, 'ê²½ë¡œë‹¹': 8, 'ì‹¤ë²„': 8,
        'ìš”ì–‘ì›': 9, 'ìš”ì–‘ì‹œì„¤': 8, 'ì¬ê°€ìš”ì–‘': 7,
        
        # ìë…€/ê°€ì¡± ê´€ë ¨
        'êµìœ¡': 6, 'ëŒ€í•™': 6, 'ì·¨ì—…': 7, 'ê²°í˜¼': 6, 'ìœ¡ì•„': 5,
        'ì†ì': 6, 'ì†ë…€': 6, 'ë©°ëŠë¦¬': 5, 'ì‚¬ìœ„': 5,
        
        # ê¸°íƒ€
        'AI': 4, 'ìŠ¤í¬ì¸ ': 4, 'ë¬¸í™”': 4, 'ì—¬í–‰': 5, 'ì¢…êµ': 5
    }
    
    # 50/60/70ëŒ€ íŠ¹í™” ë³´ë„ˆìŠ¤ í‚¤ì›Œë“œ
    SENIOR_BONUS_KEYWORDS = {
        'ë…¸ì¸': 5, 'ì‹œë‹ˆì–´': 5, '50ëŒ€': 4, '60ëŒ€': 5, '70ëŒ€': 6,
        'ì€í‡´': 5, 'ì •ë…„': 5, 'í‡´ì§': 5, 'ì¤‘ë…„': 4, 'ë…¸ë…„': 5,
        'ë² ì´ë¹„ë¶€ë¨¸': 4, 'ì‹¤ë²„': 4, 'ê³ ë ¹': 4, 'ì¥ë…„': 3,
        'ì–´ë¥´ì‹ ': 4, 'ë…¸ì¸ì¥': 3, 'í• ë¨¸ë‹ˆ': 3, 'í• ì•„ë²„ì§€': 3
    }
    
    # MZì„¸ëŒ€/ì Šì€ì¸µ ê´€ë ¨ ì œì™¸ í‚¤ì›Œë“œ ê°•í™”
    EXCLUDE_KEYWORDS = {
        'K-POP': -5, 'ì•„ì´ëŒ': -5, 'ë°©íƒ„ì†Œë…„ë‹¨': -4, 'BTS': -4,
        'ê²Œì„': -4, 'ì˜¨ë¼ì¸ê²Œì„': -4, 'eìŠ¤í¬ì¸ ': -4,
        'ìœ íŠœë²„': -4, 'ì¸í”Œë£¨ì–¸ì„œ': -4, 'í¬ë¦¬ì—ì´í„°': -3,
        'SNS': -3, 'í‹±í†¡': -4, 'ì¸ìŠ¤íƒ€ê·¸ë¨': -3, 'í˜ì´ìŠ¤ë¶': -2,
        'MZì„¸ëŒ€': -4, 'Zì„¸ëŒ€': -4, 'ë°€ë ˆë‹ˆì–¼': -3,
        'í™í•©': -3, 'ë˜í¼': -3, 'EDM': -3,
        'ì›¹íˆ°': -2, 'ë§Œí™”': -2, 'ì• ë‹ˆë©”ì´ì…˜': -2
    }

    for news in news_list:
        score = 0
        title = news.get('title', '')
        source_rss = news.get('source_rss', '')

        # 1. ì¹´í…Œê³ ë¦¬ ê°€ì¤‘ì¹˜
        for cat, weight in SENIOR_CATEGORY_WEIGHTS.items():
            if cat in source_rss:
                score += weight
                break

        # 2. í•µì‹¬ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
        for keyword, weight in SENIOR_KEYWORD_WEIGHTS.items():
            if keyword in title:
                score += weight

        # 3. ì‹œë‹ˆì–´ íŠ¹ë³„ ê´€ì‹¬ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤
        for keyword, bonus in SENIOR_BONUS_KEYWORDS.items():
            if keyword in title:
                score += bonus

        # 4. ì œì™¸ í‚¤ì›Œë“œ í˜ë„í‹°
        for keyword, penalty in EXCLUDE_KEYWORDS.items():
            if keyword in title:
                score += penalty

        # 5. ìµœì‹ ì„± ê°€ì¤‘ì¹˜
        published_time = news.get('published_parsed')
        if published_time:
            published_dt = datetime.fromtimestamp(time.mktime(published_time))
            if datetime.now() - published_dt < timedelta(days=1):
                score += 3  # ìµœì‹ ì„± ê°€ì¤‘ì¹˜ ì¦ê°€

            # 6. ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜ (50/60/70ëŒ€ ìƒí™œíŒ¨í„´ ë°˜ì˜)
            hour = published_dt.hour
            if 6 <= hour <= 9:  # ì•„ì¹¨ ë‰´ìŠ¤ ì‹œê°„
                score += 2
            elif 12 <= hour <= 14:  # ì ì‹¬ì‹œê°„
                score += 1
            elif 18 <= hour <= 21:  # ì €ë… ë‰´ìŠ¤ ì‹œê°„
                score += 1.5

        news['score'] = score

    sorted_news = sorted(news_list, key=lambda x: x.get('score', 0), reverse=True)

    logger.info("ì‹œë‹ˆì–´(50/60/70ëŒ€) ë§ì¶¤ ë‰´ìŠ¤ ì¤‘ìš”ë„ í‰ê°€ ë° ìƒìœ„ ì„ ì •...")
    for i, news in enumerate(sorted_news[:10]):
        logger.info(f"  - {i+1}ìœ„ (ì ìˆ˜: {news.get('score', 0):.1f}): {news.get('title', '')}")

    return sorted_news[:num_to_select]

def create_senior_parody_with_claude(news_item: Dict[str, Any], existing_titles: List[str]) -> str:
    """Claude AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œë‹ˆì–´ ë‰´ìŠ¤ íŒ¨ëŸ¬ë”” ìƒì„±"""
    client = Anthropic(api_key=CLAUDE_API_KEY)

    news_title = news_item.get('title', 'ì œëª© ì—†ìŒ')
    news_summary = news_item.get('text', '')[:3000]  # 5000ì—ì„œ 3000ìœ¼ë¡œ ë‹¨ì¶•

    # ì¤‘ë³µ ë°©ì§€ ëª©ë¡ ë¬¸ìì—´ ìƒì„±
    if existing_titles:
        existing_titles_str = "- " + "\n- ".join(existing_titles[-10:])  # ìµœê·¼ 10ê°œë§Œ í‘œì‹œ
    else:
        existing_titles_str = "ì—†ìŒ"

    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    parody_prompt = f"""
ë‹¹ì‹ ì€ 50~70ëŒ€ ì‹œë‹ˆì–´ ì„¸ëŒ€ë¥¼ ìœ„í•œ ë‰´ìŠ¤ íŒ¨ëŸ¬ë”” ì½˜í…ì¸  í¬ë¦¬ì—ì´í„°ì…ë‹ˆë‹¤. ì œê³µëœ ë‰´ìŠ¤ ê¸°ì‚¬ë¡œ ì‹œë‹ˆì–´ë“¤ì´ ê³µê°í•  ìˆ˜ ìˆëŠ” íŒ¨ëŸ¬ë””ë¥¼ ë§Œë“œì„¸ìš”.

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. (ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ê¸ˆì§€)
{{
  "ou_title": "í›„í‚¹ ìˆëŠ” íŒ¨ëŸ¬ë”” ì œëª©(30ì ì´ë‚´)",
  "latte": "ìš°ë¦¬ ë•ŒëŠ”... í˜•ì‹ì˜ ê³¼ê±° íšŒìƒ + í˜„ì¬ ìƒí™© ë¹„êµ(100ì ì´ë‚´)",
  "ou_think": "ì‹œë‹ˆì–´ ê´€ì ì˜ í˜„ì‹¤ì  ê±±ì •ê³¼ ê³µê° + ì•½ê°„ì˜ ìœ„íŠ¸(80ì ì´ë‚´)"
}}

[ì‘ì„± ê°€ì´ë“œë¼ì¸]
- ou_title: ê°íƒ„ì‚¬/ê¶ê¸ˆì¦/ê°ì • í‘œí˜„ í™œìš©, 3ì´ˆ ë‚´ í´ë¦­ ìœ ë„, ì§§ê³  ì„íŒ©íŠ¸ ìˆê²Œ
- latte: "ìš°ë¦¬ ë•ŒëŠ”/ì‹œì ˆì—”..."ìœ¼ë¡œ ì‹œì‘, êµ¬ì²´ì  ê³¼ê±° ê²½í—˜+í˜„ì¬ ëŒ€ë¹„, 50~100ì
- ou_think: í˜„ì‹¤ì  ê±±ì •, ìë…€ ì„¸ëŒ€ ìš°ë ¤, ìœ„íŠ¸/ì²´ë… ì„ì¸ ìì—°ìŠ¤ëŸ¬ìš´ ì–´ë¯¸, 60~100ì
- ê°ì • í†¤: 70% ê³µê°/í˜„ì‹¤, 20% ìœ„íŠ¸, 10% í¬ë§, ê³¼ë„í•œ ë¶€ì •/ì •ì¹˜ì„±/ì„¸ëŒ€ê°ˆë“±/ë¯¸í™” ê¸ˆì§€

[ì˜ˆì‹œ]
ë‰´ìŠ¤: "êµ­ê³ ì±„ ê¸ˆë¦¬ 2.4% ê¸°ë¡"
{{
  "ou_title": "ì€í–‰ ì´ì 2.4%... ë¬¼ê°€ 7% ì–´ì©Œë‚˜",
  "latte": "ìš°ë¦¬ ë•ŒëŠ” ì •ê¸°ì˜ˆê¸ˆ ì´ìê°€ 20%ì˜€ëŠ”ë° ì§€ê¸ˆì€ ê³ ì‘ 2%ëŒ€ë¼ë‹ˆ. í‰ìƒ ëª¨ì€ ëˆì´ ë¬¼ê°€ ë•Œë¬¸ì— ë°˜í† ë§‰ ë‚˜ê² ì–´",
  "ou_think": "ë¬¼ê°€ëŠ” ì²œì •ë¶€ì§€ë¡œ ì˜¤ë¥´ëŠ”ë° ì´ìëŠ” ë°”ë‹¥... ë…¸í›„ìê¸ˆ ë‹¤ ë…¹ê² ë„¤. ì €ì¶•í•´ë´¤ì ì†í•´ë¼ë‹ˆ ì´ê²Œ ë¬´ìŠ¨ ì„¸ìƒì´ì•¼"
}}

[ì¤‘ë³µ ë°©ì§€]
- ì´ë¯¸ ìƒì„±ëœ ì œëª©ë“¤ê³¼ ì¤‘ë³µë˜ì§€ ì•Šê²Œ ìƒˆë¡­ê³  ì°½ì˜ì ìœ¼ë¡œ ë§Œë“œì„¸ìš”.
- ìµœê·¼ ìƒì„±ëœ ì œëª© ëª©ë¡:
{existing_titles_str}

[ê¸ˆì§€ì‚¬í•­]
- ì´ëª¨ì§€, ì Šì€ ì„¸ëŒ€ ìš©ì–´, ê³¼ë„í•œ ì •ì¹˜ì„±, 30ì ì´ìƒ ì œëª©, JSON ì™¸ í…ìŠ¤íŠ¸, ê°€ì§œë‰´ìŠ¤/ì„ ë™ ê¸ˆì§€

[ë‰´ìŠ¤ ê¸°ì‚¬]
- ì œëª©: {news_title}
- ë‚´ìš©: {news_summary[:800]}
"""

    from anthropic.types import MessageParam
    messages: List[MessageParam] = [
        {"role": "user", "content": parody_prompt}
    ]

    max_retries = 3
    retry_delay = 5  # seconds

    for attempt in range(max_retries):
        try:
            response = client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=2000,  # 1500ì—ì„œ 2000ìœ¼ë¡œ ì¦ê°€
                temperature=0.8,
                messages=messages
            )

            response_text = ""
            if response.content:
                first_content = response.content[0]
                if hasattr(first_content, 'text') and hasattr(first_content, 'type') and first_content.type == 'text':
                    response_text = first_content.text
                elif isinstance(first_content, dict) and 'text' in first_content:
                    response_text = first_content['text']
                else:
                    response_text = str(first_content)

            if not response_text:
                logger.warning("Claude ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                continue

            return response_text

        except (APIError, OverloadedError) as e:
            error_message = str(e)
            if 'credit balance is too low' in error_message:
                logger.error("ğŸš¨ Claude API í¬ë ˆë”§ ë¶€ì¡±! Anthropic Consoleì—ì„œ í¬ë ˆë”§ì„ ì¶©ì „í•´ì£¼ì„¸ìš”.")
                logger.error("ğŸ”— https://console.anthropic.com/")
                return ""
            elif 'rate limit' in error_message.lower():
                logger.warning(f"API ì†ë„ ì œí•œì— ê±¸ë ¸ìŠµë‹ˆë‹¤. {retry_delay}ì´ˆ í›„ ì¬ì‹œë„... ({attempt + 1}/{max_retries})")
                time.sleep(retry_delay)
                retry_delay *= 2
            elif isinstance(e, OverloadedError):
                if attempt < max_retries - 1:
                    logger.warning(f"Claude AIê°€ ê³¼ë¶€í•˜ ìƒíƒœì…ë‹ˆë‹¤. {retry_delay}ì´ˆ í›„ ì¬ì‹œë„... ({attempt + 1}/{max_retries})")
                    time.sleep(retry_delay)
                    retry_delay *= 2  # Exponential backoff
                else:
                    logger.error(f"Claude AI ê³¼ë¶€í•˜ë¡œ íŒ¨ëŸ¬ë”” ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
                    return ""
            else:
                logger.error(f"Claude API ì˜¤ë¥˜: {e}")
                return ""
        except Exception as e:
            logger.error(f"Claude AI ìš”ì²­ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""
    return ""

def save_results_to_gsheet(client, parody_data_list: List[Dict[str, Any]], spreadsheet_id: str, worksheet_name: str):
    """ìƒì„±ëœ íŒ¨ëŸ¬ë”” ê²°ê³¼ë¥¼ êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥í•©ë‹ˆë‹¤. (ì´ì „ ê¸°ë¡ ì‚­ì œ í›„ ìƒˆë¡œ ê¸°ë¡)"""
    try:
        spreadsheet = client.open_by_key(spreadsheet_id)
        try:
            worksheet = spreadsheet.worksheet(worksheet_name)
        except gspread.WorksheetNotFound:
            # ì›Œí¬ì‹œíŠ¸ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ë§Œë“­ë‹ˆë‹¤.
            worksheet = spreadsheet.add_worksheet(title=worksheet_name, rows=1, cols=20)
            logger.info(f"ìƒˆ ì›Œí¬ì‹œíŠ¸ '{worksheet_name}'ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

        # í—¤ë” ì •ì˜
        headers = ['today', 'ou_title', 'original_title', 'latte', 'ou_think', 'disclaimer', 'source_url']
        
        if not parody_data_list:
            logger.info("êµ¬ê¸€ ì‹œíŠ¸ì— ì¶”ê°€í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            # ë°ì´í„°ê°€ ì—†ì–´ë„ í—¤ë”ë§Œ ë‚¨ê¸°ê³  ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
            worksheet.clear()
            worksheet.append_row(headers)
            logger.info("ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚­ì œí•˜ê³  í—¤ë”ë§Œ ë‚¨ê²¼ìŠµë‹ˆë‹¤.")
            return
            
        # ìƒˆë¡œìš´ ë°ì´í„° ì¤€ë¹„
        rows_to_upload = [headers]  # í—¤ë”ë¥¼ ì²« ë²ˆì§¸ í–‰ìœ¼ë¡œ ì¶”ê°€
        today_str = get_kst_now().strftime('%Y-%m-%d, %a').lower()

        for p_data in parody_data_list:
            # original_linkê°€ ì—†ìœ¼ë©´ url í•„ë“œë„ ë°±ì—…ìœ¼ë¡œ ì €ì¥
            source_url = p_data.get('original_link', p_data.get('url', ''))
            row = [
                today_str,
                p_data.get('ou_title', ''),
                p_data.get('original_title', ''),
                p_data.get('latte', ''),
                p_data.get('ou_think', ''),
                DISCLAIMER,
                source_url
            ]
            rows_to_upload.append(row)
        
        # ê¸°ì¡´ ë°ì´í„° ëª¨ë‘ ì‚­ì œ í›„ ìƒˆë¡œìš´ ë°ì´í„°ë¡œ êµì²´
        worksheet.clear()
        worksheet.update('A1', rows_to_upload)
        logger.info(f"êµ¬ê¸€ ì‹œíŠ¸ '{worksheet_name}'ì˜ ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚­ì œí•˜ê³  {len(parody_data_list)}ê°œ ìƒˆë¡œìš´ ë°ì´í„°ë¡œ êµì²´ ì™„ë£Œ!")

    except Exception as e:
        logger.error(f"êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def main():
    start_time = time.time()
    print("="*50)
    print("ì‹œë‹ˆì–´(50/60/70ëŒ€) ë‰´ìŠ¤ íŒ¨ëŸ¬ë”” ìë™ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.", flush=True)
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", flush=True)
    print("="*50)

    # 1. ì„¤ì • ë¡œë“œ
    logger.info("ì„¤ì • íŒŒì¼(asset/rawdata.txt) ë¡œë“œ ì¤‘...")
    config = parse_rawdata(str(SCRIPT_DIR / 'asset/rawdata.txt'))
    if not config or not config.get('rss_urls'):
        logger.error("ì„¤ì • íŒŒì¼ì— [ì—°í•©ë‰´ìŠ¤RSS] ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    if 'íŒ¨ëŸ¬ë””ê²°ê³¼_ìŠ¤í”„ë ˆë“œì‹œíŠ¸_ID' not in config:
        logger.error("ì„¤ì • íŒŒì¼ì— 'íŒ¨ëŸ¬ë””ê²°ê³¼_ìŠ¤í”„ë ˆë“œì‹œíŠ¸_ID' ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # 2. RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    logger.info("RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    all_news_entries = fetch_news_from_rss(config['rss_urls'])
    
    if not all_news_entries:
        logger.error("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. RSS í”¼ë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    # 3. ë‰´ìŠ¤ ì¤‘ìš”ë„ í‰ê°€ ë° ì„ íƒ
    # ì¶©ë¶„íˆ ë§ì€ í›„ë³´êµ° í™•ë³´ (ì˜ˆ: 100ê°œ)
    candidate_count = 100
    sorted_news = rank_and_select_news(all_news_entries, candidate_count)
    
    # 4. ìµœì†Œ 30ê°œ íŒ¨ëŸ¬ë””ê°€ ë‚˜ì˜¬ ë•Œê¹Œì§€ ë°˜ë³µ
    logger.info("ìµœì†Œ 30ê°œ íŒ¨ëŸ¬ë””ê°€ ë‚˜ì˜¬ ë•Œê¹Œì§€ ë°˜ë³µ ìƒì„±...")
    parody_results = []
    existing_titles = []
    max_needed = 30
    api_failures = 0
    max_failures = 5
    
    for news in sorted_news:
        # ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘
        article = get_article_content(news.get('link', ''))
        if not article or not article.get('text'):
            continue
        article['source_rss'] = news.get('source_rss')
        article['original_link'] = news.get('link', '')
        
        # Claude íŒ¨ëŸ¬ë”” ìƒì„±
        try:
            parody_response = create_senior_parody_with_claude(article, existing_titles)
            if not parody_response:
                api_failures += 1
                logger.warning(f"Claude ì‘ë‹µì´ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤. (ì‹¤íŒ¨ íšŸìˆ˜: {api_failures})")
                if api_failures >= max_failures:
                    logger.error(f"ì—°ì† {max_failures}íšŒ API í˜¸ì¶œ ì‹¤íŒ¨ë¡œ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
                continue

            clean_str = parody_response.strip()
            if clean_str.startswith("```json"):
                clean_str = clean_str[7:].strip()
            elif clean_str.startswith("```"):
                clean_str = clean_str[3:].strip()
            if clean_str.endswith("```"):
                clean_str = clean_str[:-3].strip()
            json_start = clean_str.find('{')
            json_end = clean_str.rfind('}')
            if json_start != -1 and json_end != -1 and json_end > json_start:
                clean_str = clean_str[json_start:json_end+1]
            clean_str = re.sub(r'\s+', ' ', clean_str).strip()
            try:
                parody_data = json.loads(clean_str)
                if 'ou_title' not in parody_data:
                    logger.warning("'ou_title' í‚¤ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                current_title = parody_data['ou_title']
                is_duplicate = False
                for title in existing_titles:
                    if SequenceMatcher(None, current_title, title).ratio() > 0.85:
                        is_duplicate = True
                        logger.warning(f"ìœ ì‚¬í•œ ì œëª©ì´ ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ê±´ë„ˆëœë‹ˆë‹¤: {current_title}")
                        break
                if not is_duplicate:
                    parody_data['original_title'] = article['title']
                    parody_data['original_link'] = article['url']
                    parody_results.append(parody_data)
                    existing_titles.append(current_title)
                    logger.info(f"âœ… íŒ¨ëŸ¬ë”” ìƒì„± ì„±ê³µ: {current_title}")
                    api_failures = 0
            except json.JSONDecodeError as e:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                logger.warning(f"ì •ë¦¬ëœ ì‘ë‹µ: {clean_str[:100]}...")
                continue
        except Exception as e:
            logger.error(f"íŒ¨ëŸ¬ë”” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
        if len(parody_results) >= max_needed:
            break
    logger.info(f"ì´ {len(parody_results)}ê°œì˜ ê³ ìœ í•œ íŒ¨ëŸ¬ë””ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

    # 6. êµ¬ê¸€ ì‹œíŠ¸ì— ê²°ê³¼ ì €ì¥
    logger.info("ìƒì„±ëœ íŒ¨ëŸ¬ë”” ê²°ê³¼ë¥¼ êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥ ì¤‘...")
    try:
        g_client = get_gspread_client()
        save_results_to_gsheet(g_client, parody_results, config['íŒ¨ëŸ¬ë””ê²°ê³¼_ìŠ¤í”„ë ˆë“œì‹œíŠ¸_ID'], WRITE_SHEET_NAME)
    except Exception as e:
        logger.error(f"êµ¬ê¸€ ì¸ì¦ ë˜ëŠ” ì‹œíŠ¸ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")

    # 7. ì¢…ë£Œ
    end_time = time.time()
    print("\n" + "="*50)
    print("ëª¨ë“  ì‘ì—… ì™„ë£Œ!", flush=True)
    print(f"ì´ ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ", flush=True)
    print(f"ì„±ê³µì ìœ¼ë¡œ ìƒì„±ëœ íŒ¨ëŸ¬ë””: {len(parody_results)}ê°œ", flush=True)
    print("="*50)

if __name__ == "__main__":
    main() 