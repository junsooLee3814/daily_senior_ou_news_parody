import os
import feedparser
from datetime import datetime, timedelta
from anthropic import Anthropic, OverloadedError
from dotenv import load_dotenv
from common_utils import get_gsheet, get_gspread_client
from difflib import SequenceMatcher
import json
import re
from pathlib import Path
import time
import gspread
import pandas as pd
from newspaper import Article, Config
from concurrent.futures import ThreadPoolExecutor, as_completed

# .env íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¥¼ ì§€ì •í•˜ì—¬ ë¡œë“œ
env_path = Path('.') / '.env'
load_dotenv(dotenv_path=env_path, verbose=True)

# Claude AI API í‚¤
CLAUDE_API_KEY = os.getenv('CLAUDE_API_KEY')
if not CLAUDE_API_KEY:
    raise ValueError("CLAUDE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì˜ í˜„ì¬ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
SCRIPT_DIR = Path(__file__).resolve().parent

def parse_rawdata(file_path='asset/rawdata.txt'):
    """rawdata.txt íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ ì„¤ì •ê°’ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    config = {'rss_urls': []}
    current_section = None
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                if line.startswith('[') and line.endswith(']'):
                    current_section = line[1:-1]
                elif current_section == 'ì—°í•©ë‰´ìŠ¤RSS':
                    config['rss_urls'].append(line)
                elif ':' in line:
                    key, value = line.split(':', 1)
                    config[key.strip()] = value.strip()
    except FileNotFoundError:
        print(f"ì˜¤ë¥˜: ì„¤ì • íŒŒì¼({file_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    except Exception as e:
        print(f"ì˜¤ë¥˜: ì„¤ì • íŒŒì¼({file_path}) íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None
    return config

# êµ¬ê¸€ ì‹œíŠ¸ ì„¤ì •
WRITE_SHEET_NAME = 'senior_ou_news_parody_v3'
DISCLAIMER = "ë©´ì±…ì¡°í•­ : íŒ¨ëŸ¬ë””/íŠ¹ì •ê¸°ê´€,ê°œì¸ê³¼ ë¬´ê´€/íˆ¬ìì¡°ì–¸ì•„ë‹˜/ì¬ë¯¸ëª©ì "
SOURCE_PREFIX = "https://gnews/" # ì¶œì²˜ URL ì ‘ë‘ì‚¬

def get_article_content(url):
    """ì£¼ì–´ì§„ URLì˜ ë‰´ìŠ¤ ë³¸ë¬¸ì„ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤."""
    try:
        user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'
        config = Config()
        config.browser_user_agent = user_agent
        config.request_timeout = 10
        
        article = Article(url, config=config)
        article.download()
        article.parse()
        return {'url': url, 'title': article.title, 'text': article.text, 'publish_date': article.publish_date}
    except Exception as e:
        print(f"  - (ê²½ê³ ) ê¸°ì‚¬ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url}, ì˜¤ë¥˜: {e}")
        return None

def fetch_news_from_rss(rss_urls):
    """ì—¬ëŸ¬ RSS í”¼ë“œì—ì„œ ìµœì‹  ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    all_entries = []
    for url in rss_urls:
        print(f"  - RSS í”¼ë“œ í™•ì¸ ì¤‘: {url}")
        feed = feedparser.parse(url)
        for entry in feed.entries:
            entry['source_rss'] = url
            all_entries.append(entry)
    # ì¤‘ë³µ ì œê±° (link ê¸°ì¤€)
    unique_entries = list({entry.link: entry for entry in all_entries}.values())
    print(f"  -> ì´ {len(unique_entries)}ê°œì˜ ê³ ìœ í•œ ë‰´ìŠ¤ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
    return unique_entries

def rank_and_select_news(news_list, num_to_select=30):
    """ë‰´ìŠ¤ ëª©ë¡ì˜ ì¤‘ìš”ë„ë¥¼ í‰ê°€í•˜ê³  ìƒìœ„ Nê°œë¥¼ ì„ íƒí•©ë‹ˆë‹¤."""
    
    CATEGORY_WEIGHTS = {
        'opinion': 1.5, 'politics': 1.4, 'economy': 1.3, 'market': 1.2,
        'local': 1.0, 'health': 0.9
    }
    KEYWORD_WEIGHTS = {
        'ê¸ˆë¦¬': 5, 'ì •ë¶€': 4, 'íŠ¹ê²€': 4, 'ëŒ€í†µë ¹': 4, 'AI': 3, 'ì—°ê¸ˆ': 3,
        'ë¶€ë™ì‚°': 3, 'ë¬¼ê°€': 2
    }

    for news in news_list:
        score = 0
        # 1. ì¹´í…Œê³ ë¦¬ ê°€ì¤‘ì¹˜
        for cat, weight in CATEGORY_WEIGHTS.items():
            if cat in news.get('source_rss', ''):
                score += weight
                break
        
        # 2. í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
        title = news.get('title', '')
        for keyword, weight in KEYWORD_WEIGHTS.items():
            if keyword in title:
                score += weight
        
        # 3. ìµœì‹ ì„± ê°€ì¤‘ì¹˜ (ìµœê·¼ 24ì‹œê°„ ë‚´ ê¸°ì‚¬ì— ê°€ì‚°ì )
        published_time = news.get('published_parsed')
        if published_time:
            published_dt = datetime.fromtimestamp(time.mktime(published_time))
            if datetime.now() - published_dt < timedelta(days=1):
                score += 2

        news['score'] = score
    
    # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    sorted_news = sorted(news_list, key=lambda x: x.get('score', 0), reverse=True)
    
    print("\n[2/6] ë‰´ìŠ¤ ì¤‘ìš”ë„ í‰ê°€ ë° ìƒìœ„ 30ê°œ ì„ ì •...")
    for i, news in enumerate(sorted_news[:10]): # ìƒìœ„ 10ê°œë§Œ ì ìˆ˜ í‘œì‹œ
        print(f"  - {i+1}ìœ„ (ì ìˆ˜: {news.get('score', 0):.1f}): {news.title}")
        
    return sorted_news[:num_to_select]

def create_senior_parody_with_claude(news_item, existing_titles):
    """Claude AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œë‹ˆì–´ ë‰´ìŠ¤ íŒ¨ëŸ¬ë”” ìƒì„±"""
    client = Anthropic(api_key=CLAUDE_API_KEY)

    news_title = news_item.get('title', 'ì œëª© ì—†ìŒ')
    news_summary = news_item.get('text', '')[:5000]

    # f-string ë‚´ ë°±ìŠ¬ë˜ì‹œ ë¬¸ë²• ì˜¤ë¥˜ë¥¼ í”¼í•˜ê¸° ìœ„í•´, ì¤‘ë³µ ë°©ì§€ ëª©ë¡ ë¬¸ìì—´ì„ ë¯¸ë¦¬ ìƒì„±í•©ë‹ˆë‹¤.
    if existing_titles:
        # ê° ì œëª© ì•ì— "- "ë¥¼ ë¶™ì´ê³  ì¤„ë°”ê¿ˆìœ¼ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.
        existing_titles_str = "- " + "\n- ".join(existing_titles)
    else:
        existing_titles_str = "ì—†ìŒ"

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    parody_prompt = f"""# ì˜¤ëŠ˜ì˜ìœ ë¨¸_ë‰´ìŠ¤íŒ¨ëŸ¬ë””_ì‹œë‹ˆì–´V ì œì‘ ì§€ì¹¨

## ğŸ¯ **ë¯¸ì…˜**
ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ì¸µì„ ìœ„í•œ ì˜¤ëŠ˜ì˜ìœ ë¨¸ ìŠ¤íƒ€ì¼ ë‰´ìŠ¤ íŒ¨ëŸ¬ë”” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µë˜ëŠ” **ë‰´ìŠ¤ ë³¸ë¬¸ ì „ì²´**ë¥¼ ê¹Šì´ ìˆê²Œ ë¶„ì„í•˜ì—¬, ì‹œë‹ˆì–´ë“¤ì´ ê³µê°í•˜ê³  ì¦ê¸¸ ìˆ˜ ìˆëŠ” ìœ ë¨¸ëŸ¬ìŠ¤í•œ íŒ¨ëŸ¬ë””ë¥¼ ì œì‘í•´ì•¼ í•©ë‹ˆë‹¤.

## ğŸ“° ë¶„ì„í•  ë‰´ìŠ¤ ì›ë¬¸
- **ì œëª©:** {news_title}
- **ë³¸ë¬¸:**
{news_summary}

---
## ğŸ“ **ê²°ê³¼ë¬¼ í¬ë§· (ì—„ìˆ˜)**
- **(ì ˆëŒ€ ê·œì¹™) ë‹¤ë¥¸ ì„¤ëª… ì—†ì´, ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.**
- **(ì ˆëŒ€ ê·œì¹™) ì ˆëŒ€ë¡œ ì´ëª¨ì§€(emoji)ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**
- **(ì ˆëŒ€ ê·œì¹™) ê° í•­ëª©ì˜ ê¸€ììˆ˜ ì œí•œì„ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš”.**

```json
{{
  "ou_title": "ì—¬ê¸°ì— [ì˜¤ìœ _Title] ë‚´ìš© (50ì ì´ë‚´)",
  "latte": "ì—¬ê¸°ì— [ë¼ë–¼] ë‚´ìš© (100ì ì´ë‚´)",
  "ou_think": "ì—¬ê¸°ì— [ì˜¤ìœ _Think] ë‚´ìš© (70ì ì´ë‚´)"
}}
```

## ğŸ¨ **ì„¸ë¶€ ì œì‘ ê°€ì´ë“œë¼ì¸**

### **[ou_title] ì‘ì„±ë²• (50ì ì´ë‚´):**
- **ê°•ë ¥í•œ í›„í‚¹**: "ê²½ì•…", "ì†ë³´", "ì¶©ê²©", "ê¹œë†€", "ëŒ€ë°•", "í—‰!", "ì§„ì§œ?", "ë¯¸ì³¤ë‹¤", "ì„¸ìƒì—" ë“± ë§¤ë²ˆ ë‹¤ë¥¸ ê°•ë ¥í•œ í›„í‚¹ ë¬¸êµ¬ë¥¼ ì°½ì˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”. **"ì¶©ê²©!!"ë§Œ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.**
- **í•µì‹¬ ì°Œë¥´ê¸°**: ë‰´ìŠ¤ ë³¸ë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ê¿°ëš«ëŠ” ì§ˆë¬¸ì´ë‚˜ ê°íƒ„ì‚¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. (ì˜ˆ: "ì´ê±¸ ì´ì œ ì™€ì„œ í•œë‹¤ê³ ??")

### **[ë¼ë–¼] ì‘ì„±ë²• (100ì ì´ë‚´):**
- **"ìš°ë¦¬ ë•ŒëŠ”..."**: ë³¸ë¬¸ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ê³¼ê±° ê²½í—˜ì„ í˜„ì¬ì™€ ë¹„êµí•˜ë©° ì„¸ëŒ€ ê³µê°ì„ ìœ ë„í•˜ì„¸ìš”. (ì˜ˆ: "ìš°ë¦¬ ë•ŒëŠ” ì£¼íŒì•Œ íŠ€ê¸°ë˜ ê²Œ AI ê³„ì‚°ê¸°ê°€ ëë„¤...")
- **ì¹œê·¼í•œ ë§íˆ¬**: "~ë‹¤ë‹ˆ", "~ë¼ë‹ˆ", "~êµ¬ë‚˜" ë“± ìì—°ìŠ¤ëŸ¬ìš´ ê²½ì–´ì²´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

### **[ì˜¤ìœ _Think] ì‘ì„±ë²• (70ì ì´ë‚´):**
- **ì„œë¯¼ì˜ ì‹œê°**: ë‰´ìŠ¤ ë‚´ìš©ì„ ë¼ë©´ê°’, ê¸°ë¦„ê°’, ìì‹ ê±±ì • ë“± ì„œë¯¼ì˜ ì‚¶ê³¼ ì—°ê²°í•˜ì—¬ ëƒ‰ì†Œì ì´ê³  í˜„ì‹¤ì ì¸ ìƒê°ì„ í‘œí˜„í•˜ì„¸ìš”. (ì˜ˆ: "ì–´ì°¨í”¼ ì˜¬ë¼ë„ ë‚´ ì›”ê¸‰ì€ ê·¸ëŒ€ë¡œ... ã… ã… ã… ")
- **ì§ì„¤ì  í‘œí˜„**: "ê²°êµ­ ~", "ë˜ ~", "ì§„ì‘ì— ~" ê°™ì€ ì§ì„¤ì ì¸ ì–´íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

### **ê°ì • í‘œí˜„ í•„ìˆ˜ ì‚¬ìš©:**
- ë‰´ìŠ¤ë§ˆë‹¤ ??, !!, ..., ã…‹ã…‹ã…‹, ã… ã… ã…  ë“±ì„ 2~3ê°œ ì´ìƒ ìì—°ìŠ¤ëŸ½ê²Œ ì¡°í•©í•˜ì—¬ ê°ì •ì„ í’ë¶€í•˜ê²Œ í‘œí˜„í•´ì£¼ì„¸ìš”.

## âœï¸ **(ë§¤ìš° ì¤‘ìš”) ì¤‘ë³µ íŒ¨ëŸ¬ë”” ë°©ì§€**
- ì•„ë˜ëŠ” ì´ë¯¸ ìƒì„±ëœ íŒ¨ëŸ¬ë”” ì œëª©ë“¤ì…ë‹ˆë‹¤.
- **ì ˆëŒ€ë¡œ ì•„ë˜ ëª©ë¡ê³¼ ìœ ì‚¬í•œ ë‚´ìš©ì´ë‚˜ ìŠ¤íƒ€ì¼ì˜ `ou_title`ì„ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.**
- ì™„ì „íˆ ìƒˆë¡­ê³ , ì°½ì˜ì ì¸ ì œëª©ì„ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.

### ğŸ“œ ì´ë¯¸ ìƒì„±ëœ ì œëª© ëª©ë¡:
{existing_titles_str}
"""

    messages = [{"role": "user", "content": parody_prompt}]

    max_retries = 3
    retry_delay = 5  # seconds
    for attempt in range(max_retries):
        try:
            response = client.messages.create(
                model="claude-3-5-sonnet-20240620",
                max_tokens=2000,
                temperature=0.8,
                messages=messages
            )
            return response.content[0].text if response.content else ""
        except OverloadedError as e:
            if attempt < max_retries - 1:
                print(f"  - (ê²½ê³ ) Claude AIê°€ ê³¼ë¶€í•˜ ìƒíƒœì…ë‹ˆë‹¤. {retry_delay}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤... ({attempt + 1}/{max_retries})")
                time.sleep(retry_delay)
                retry_delay *= 2  # Exponential backoff
            else:
                print(f"  - (ì˜¤ë¥˜) Claude AI ê³¼ë¶€í•˜ë¡œ íŒ¨ëŸ¬ë”” ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
                return ""
        except Exception as e:
            print(f"  - (ì˜¤ë¥˜) Claude AI ìš”ì²­ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""
    return ""

def save_results_to_gsheet(client, parody_data_list, spreadsheet_id, worksheet_name):
    """ìƒì„±ëœ íŒ¨ëŸ¬ë”” ê²°ê³¼ë¥¼ êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        spreadsheet = client.open_by_key(spreadsheet_id)
        try:
            worksheet = spreadsheet.worksheet(worksheet_name)
            worksheet.clear()
            print(f"  - ê¸°ì¡´ ì›Œí¬ì‹œíŠ¸ '{worksheet_name}'ì˜ ë‚´ìš©ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
        except gspread.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title=worksheet_name, rows=len(parody_data_list) + 10, cols=20)
            print(f"  - ìƒˆ ì›Œí¬ì‹œíŠ¸ '{worksheet_name}'ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
        
        headers = ['today', 'ou_title', 'original_title', 'latte', 'ou_think', 'disclaimer', 'source_url']
        worksheet.append_row(headers)
        
        rows_to_upload = []
        today_str = datetime.now().strftime('%Y-%m-%d, %a').lower()

        for p_data in parody_data_list:
            row = [
                today_str,
                p_data.get('ou_title', ''),
                p_data.get('original_title', ''),
                p_data.get('latte', ''),
                p_data.get('ou_think', ''),
                DISCLAIMER,
                p_data.get('original_link', '')
            ]
            rows_to_upload.append(row)
            
        worksheet.append_rows(rows_to_upload)
        print(f"  -> êµ¬ê¸€ ì‹œíŠ¸ '{worksheet_name}'ì— {len(rows_to_upload)}ê°œ ë°ì´í„° ì €ì¥ ì™„ë£Œ!")

    except Exception as e:
        print(f"  ! êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def main():
    start_time = time.time()
    print("="*50)
    print("ì‹œë‹ˆì–´ ë‰´ìŠ¤ íŒ¨ëŸ¬ë”” ìë™ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*50)

    # 1. ì„¤ì • ë¡œë“œ
    print("\n[1/6] ì„¤ì • íŒŒì¼(asset/rawdata.txt) ë¡œë“œ ì¤‘...")
    config = parse_rawdata(SCRIPT_DIR / 'asset/rawdata.txt')
    if not config or not config.get('rss_urls'):
        print("  ! ì„¤ì • íŒŒì¼ì— [ì—°í•©ë‰´ìŠ¤RSS] ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    if not config or 'íŒ¨ëŸ¬ë””ê²°ê³¼_ìŠ¤í”„ë ˆë“œì‹œíŠ¸_ID' not in config:
        print("  ! ì„¤ì • íŒŒì¼ì— 'íŒ¨ëŸ¬ë””ê²°ê³¼_ìŠ¤í”„ë ˆë“œì‹œíŠ¸_ID' ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # 2. RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    print("\n[2/6] RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    all_news_entries = fetch_news_from_rss(config['rss_urls'])
    
    # 3. ë‰´ìŠ¤ ì¤‘ìš”ë„ í‰ê°€ ë° ì„ íƒ
    selected_news = rank_and_select_news(all_news_entries, int(config.get('ì¹´ë“œë‰´ìŠ¤_ê°œìˆ˜', 30)))
    
    # 4. ì„ íƒëœ ë‰´ìŠ¤ì˜ ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘
    print("\n[3/6] ì„ íƒëœ ë‰´ìŠ¤ì˜ ì „ì²´ ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ ì¤‘...")
    scraped_articles = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_url = {executor.submit(get_article_content, news.link): news for news in selected_news}
        for future in as_completed(future_to_url):
            result = future.result()
            if result and result['text']:
                original_news_item = future_to_url[future]
                result['source_rss'] = original_news_item.get('source_rss')
                result['original_link'] = original_news_item.link
                scraped_articles.append(result)
                print(f"  - ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {result['title'][:30]}...")
            time.sleep(0.1) # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì•½ê°„ì˜ ë”œë ˆì´
    print(f"  -> ì´ {len(scraped_articles)}ê°œ ê¸°ì‚¬ì˜ ë³¸ë¬¸ì„ ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")

    # 5. Claude AIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ íŒ¨ëŸ¬ë”” ìƒì„±
    print("\n[4/6] Claude AIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ íŒ¨ëŸ¬ë”” ìƒì„± ì¤‘...")
    parody_results = []
    existing_titles = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_article = {executor.submit(create_senior_parody_with_claude, article, existing_titles): article for article in scraped_articles}
        
        for i, future in enumerate(as_completed(future_to_article)):
            article = future_to_article[future]
            print(f"  - íŒ¨ëŸ¬ë”” ìƒì„± ì¤‘ ({i+1}/{len(scraped_articles)}): {article['title'][:30]}...")
            
            try:
                parody_json_str = future.result()
                if not parody_json_str:
                    continue

                # Claude ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (```json ... ``` í•¸ë“¤ë§)
                clean_str = parody_json_str.strip()
                if clean_str.startswith("```json"):
                    clean_str = clean_str[7:].strip()
                if clean_str.endswith("```"):
                    clean_str = clean_str[:-3].strip()
                
                try:
                    parody_data = json.loads(clean_str)
                    
                    # ì¤‘ë³µ ë° ìœ ì‚¬ë„ ê²€ì‚¬
                    is_duplicate = False
                    if 'ou_title' in parody_data:
                        current_title = parody_data['ou_title']
                        for title in existing_titles:
                            if SequenceMatcher(None, current_title, title).ratio() > 0.85:
                                is_duplicate = True
                                print(f"  - (ê²½ê³ ) ìœ ì‚¬í•œ ì œëª©ì´ ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ê±´ë„ˆëœë‹ˆë‹¤: {current_title}")
                                break
                        
                        if not is_duplicate:
                            parody_data['original_title'] = article['title']
                            parody_data['original_link'] = article['url']
                            parody_results.append(parody_data)
                            existing_titles.append(current_title)
                    else:
                        print(f"  - (ê²½ê³ ) ì‘ë‹µì— 'ou_title'ì´ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤: {clean_str}")

                except json.JSONDecodeError:
                    print(f"  - (ê²½ê³ ) Claude AIì˜ ì‘ë‹µì´ ìœ íš¨í•œ JSONì´ ì•„ë‹™ë‹ˆë‹¤: {parody_json_str}")
                    continue

            except Exception as e:
                print(f"  - (ì˜¤ë¥˜) íŒ¨ëŸ¬ë”” ìƒì„± ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {article['title']}, {e}")

    print(f"  -> ì´ {len(parody_results)}ê°œì˜ ê³ ìœ í•œ íŒ¨ëŸ¬ë””ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

    # 6. êµ¬ê¸€ ì‹œíŠ¸ì— ê²°ê³¼ ì €ì¥
    print("\n[5/6] ìƒì„±ëœ íŒ¨ëŸ¬ë”” ê²°ê³¼ë¥¼ êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥ ì¤‘...")
    try:
        g_client = get_gspread_client()
        save_results_to_gsheet(g_client, parody_results, config['íŒ¨ëŸ¬ë””ê²°ê³¼_ìŠ¤í”„ë ˆë“œì‹œíŠ¸_ID'], WRITE_SHEET_NAME)
    except Exception as e:
        print(f"  ! êµ¬ê¸€ ì¸ì¦ ë˜ëŠ” ì‹œíŠ¸ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")

    # 7. ì¢…ë£Œ
    end_time = time.time()
    print("\n[6/6] ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print(f"ì´ ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
    print("="*50)

if __name__ == "__main__":
    main() 